name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  unit-tests:
    name: Unit Tests (Stub-based)
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
    - uses: actions/checkout@v4

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential

    - name: Configure CMake
      working-directory: tests
      run: cmake -S . -B build -DCMAKE_BUILD_TYPE=Release

    - name: Build tests
      working-directory: tests
      run: cmake --build build --config Release

    - name: Run unit tests
      working-directory: tests
      run: ./build/TestRunner --success

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results-${{ matrix.os }}
        path: tests/build/

  integration-tests-macos:
    name: Integration Tests (macOS)
    runs-on: macos-latest

    steps:
    - uses: actions/checkout@v4
      with:
        lfs: true  # Pull LFS files (xcframework binaries)

    - name: Download test models
      working-directory: tests
      run: |
        mkdir -p fixtures

        # Download tiny-random-llama (12MB) for basic integration tests
        # Using the exact URL from setup_test_model.sh
        if [ ! -f fixtures/tiny-random-llama.gguf ]; then
          echo "Downloading tiny-random-llama.gguf (12MB)..."
          curl -fsSL --retry 3 --retry-delay 2 \
            "https://huggingface.co/tensorblock/tiny-random-LlamaForCausalLM-ONNX-GGUF/resolve/main/tiny-random-LlamaForCausalLM-ONNX-Q4_K_M.gguf" \
            -o fixtures/tiny-random-llama.gguf

          # Verify it's a valid GGUF file (check magic bytes)
          echo "Verifying tiny-random-llama.gguf..."
          if ! head -c 4 fixtures/tiny-random-llama.gguf | grep -q "GGUF"; then
            echo "ERROR: Downloaded file is not a valid GGUF file"
            echo "File header:"
            head -c 32 fixtures/tiny-random-llama.gguf | xxd
            rm -f fixtures/tiny-random-llama.gguf
            exit 1
          fi
          echo "✓ tiny-random-llama.gguf verified"
        else
          echo "✓ Using cached tiny-random-llama.gguf"
        fi

        # Download TinyLlama-1.1B (638MB) for coherent model tests
        if [ ! -f fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf ]; then
          echo "Downloading tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (638MB)..."
          curl -fsSL --retry 3 --retry-delay 2 \
            "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf" \
            -o fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

          # Verify GGUF magic bytes
          echo "Verifying tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf..."
          if ! head -c 4 fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | grep -q "GGUF"; then
            echo "ERROR: Downloaded file is not a valid GGUF file"
            echo "File header:"
            head -c 32 fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | xxd
            rm -f fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
            exit 1
          fi
          echo "✓ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf verified"
        else
          echo "✓ Using cached tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
        fi

        echo "All test models ready"

    - name: Configure CMake for integration tests
      working-directory: tests
      run: |
        cmake -S . -B build_integration \
          -DLLOYAL_BUILD_INTEGRATION_TESTS=ON \
          -DCMAKE_BUILD_TYPE=Release

    - name: Build integration tests
      working-directory: tests
      run: cmake --build build_integration --config Release

    - name: Run integration tests (random model)
      working-directory: tests
      env:
        LLAMA_TEST_MODEL: ${{ github.workspace }}/packages/liblloyal/tests/fixtures/tiny-random-llama.gguf
      run: |
        echo "Running basic integration tests with tiny-random-llama..."
        ./build_integration/IntegrationRunner --success

    - name: Run integration tests (coherent model)
      working-directory: tests
      env:
        LLAMA_TEST_MODEL: ${{ github.workspace }}/packages/liblloyal/tests/fixtures/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
      run: |
        echo "Running coherent model tests with TinyLlama-1.1B..."
        ./build_integration/IntegrationRunner --success

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: integration-test-results-macos
        path: tests/build_integration/

  lint:
    name: Code Style Check
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Install clang-format
      run: sudo apt-get update && sudo apt-get install -y clang-format

    - name: Check formatting
      run: |
        find include/ tests/ -name '*.hpp' -o -name '*.cpp' | while read file; do
          clang-format --dry-run --Werror "$file" || exit 1
        done

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests-macos]
    if: always()

    steps:
    - name: Check test results
      run: |
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests (macOS): ${{ needs.integration-tests-macos.result }}"

        if [ "${{ needs.unit-tests.result }}" != "success" ] || \
           [ "${{ needs.integration-tests-macos.result }}" != "success" ]; then
          echo "❌ Some tests failed"
          exit 1
        else
          echo "✅ All tests passed"
        fi